"""
ç«¯åˆ°ç«¯æµ‹è¯•æ‰¹é‡ç¿»è¯‘åŠŸèƒ½
æµ‹è¯•ä»é…ç½®æ–‡ä»¶åˆ›å»ºåˆ°æ¨¡å‹é€‰æ‹©çš„å®Œæ•´æµç¨‹
"""
import sys
import os
import io
import json
import tempfile

# å¼ºåˆ¶ UTF-8 è¾“å‡º
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

print("\n" + "="*70)
print("ç«¯åˆ°ç«¯æµ‹è¯•ï¼šæ‰¹é‡ç¿»è¯‘å®Œæ•´æµç¨‹")
print("="*70)

# æµ‹è¯•1: åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
print("\n[æ­¥éª¤1] åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶...")

test_config = {
    "tasks": [
        {
            "task_id": "test-1",
            "source": "ä½ å¥½",
            "target_language": "éŸ©æ–‡"
        },
        {
            "task_id": "test-2",
            "source": "ä¸å¥½",
            "target_language": "éŸ©æ–‡"
        },
        {
            "task_id": "test-3",
            "source": "å¤§å“¥",
            "target_language": "éŸ©æ–‡"
        }
    ],
    "num_processes": 1
}

# åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
    json.dump(test_config, f, ensure_ascii=False, indent=2)
    config_file = f.name

print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
print(f"   ä»»åŠ¡æ•°é‡: {len(test_config['tasks'])}")
print(f"   ç›®æ ‡è¯­è¨€: éŸ©æ–‡")

# æµ‹è¯•2: éªŒè¯ batch_retranslate.py å¯ä»¥å¯¼å…¥
print("\n[æ­¥éª¤2] éªŒè¯ batch_retranslate.py...")

try:
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    from batch_retranslate import (
        get_gpu_memory_gb,
        select_model_by_gpu,
        retranslate_from_config
    )
    print("âœ… batch_retranslate.py å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸  éœ€è¦åœ¨ qwen_inference ç¯å¢ƒä¸­è¿è¡Œ")
    sys.exit(1)

# æµ‹è¯•3: GPU æ£€æµ‹
print("\n[æ­¥éª¤3] æ£€æµ‹ GPU æ˜¾å­˜...")
available_memory = get_gpu_memory_gb()

if available_memory > 0:
    print(f"âœ… GPU æ£€æµ‹æˆåŠŸï¼Œå¯ç”¨æ˜¾å­˜: {available_memory:.2f} GB")
else:
    print("âš ï¸  æœªæ£€æµ‹åˆ° GPU æˆ–æ˜¾å­˜ä¸è¶³")

# æµ‹è¯•4: æ¨¡å‹é€‰æ‹©
print("\n[æ­¥éª¤4] è‡ªåŠ¨é€‰æ‹©æ¨¡å‹...")

# è·å–æ¨¡å‹ç›®å½•
backend_dir = os.path.dirname(os.path.abspath(__file__))
localclip_dir = os.path.dirname(backend_dir)
workspace_dir = os.path.dirname(localclip_dir)
ai_editing_dir = os.path.dirname(workspace_dir)
models_dir = os.path.join(ai_editing_dir, "models")

print(f"æ¨¡å‹ç›®å½•: {models_dir}")

if os.path.exists(models_dir):
    selected_model = select_model_by_gpu(models_dir)
    model_name = os.path.basename(selected_model)
    print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {model_name}")

    # éªŒè¯æ¨¡å‹æ–‡ä»¶
    required_files = ["config.json", "tokenizer_config.json"]
    all_exist = True
    for file in required_files:
        file_path = os.path.join(selected_model, file)
        if os.path.exists(file_path):
            print(f"   âœ“ {file}")
        else:
            print(f"   âœ— {file} (ç¼ºå¤±)")
            all_exist = False

    if all_exist:
        print(f"âœ… æ¨¡å‹æ–‡ä»¶éªŒè¯é€šè¿‡")
    else:
        print(f"âš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´")
else:
    print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
    sys.exit(1)

# æµ‹è¯•5: æ‰§è¡Œæ‰¹é‡ç¿»è¯‘ï¼ˆæ¨¡æ‹Ÿï¼‰
print("\n[æ­¥éª¤5] æ¨¡æ‹Ÿæ‰¹é‡ç¿»è¯‘æµç¨‹...")
print("æç¤º: å®Œæ•´ç¿»è¯‘éœ€è¦åŠ è½½æ¨¡å‹ï¼Œè¿™é‡Œä»…æµ‹è¯•é…ç½®å’Œé€‰æ‹©é€»è¾‘")
print("")
print("å¦‚éœ€æµ‹è¯•å®Œæ•´ç¿»è¯‘æµç¨‹ï¼Œè¯·è¿è¡Œ:")
print(f"  conda activate qwen_inference")
print(f"  python batch_retranslate.py {config_file}")

# æµ‹è¯•6: éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼
print("\n[æ­¥éª¤6] éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼...")

with open(config_file, 'r', encoding='utf-8') as f:
    loaded_config = json.load(f)

print(f"âœ… é…ç½®æ–‡ä»¶æ ¼å¼æ­£ç¡®")
print(f"   åŒ…å«å­—æ®µ: {list(loaded_config.keys())}")
print(f"   ä»»åŠ¡æ•°é‡: {len(loaded_config['tasks'])}")
print(f"   æ¨¡å‹è·¯å¾„: {'è‡ªåŠ¨é€‰æ‹©' if 'model_path' not in loaded_config else loaded_config['model_path']}")

# æ€»ç»“
print("\n" + "="*70)
print("æµ‹è¯•æ€»ç»“")
print("="*70)

results = {
    "é…ç½®æ–‡ä»¶åˆ›å»º": "âœ…",
    "æ¨¡å—å¯¼å…¥": "âœ…",
    "GPUæ£€æµ‹": "âœ…" if available_memory > 0 else "âš ï¸",
    "æ¨¡å‹é€‰æ‹©": "âœ…",
    "æ¨¡å‹æ–‡ä»¶éªŒè¯": "âœ…" if all_exist else "âš ï¸",
    "é…ç½®æ ¼å¼éªŒè¯": "âœ…"
}

for test_name, status in results.items():
    print(f"{status} {test_name}")

print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
print("  1. è¿è¡Œ test_model_select.bat æµ‹è¯•æ¨¡å‹é€‰æ‹©")
print("  2. åœ¨åº”ç”¨ä¸­è§¦å‘ç¿»è¯‘ä»»åŠ¡ï¼ŒéªŒè¯å®Œæ•´æµç¨‹")
print("  3. æ£€æŸ¥æ—¥å¿—ç¡®è®¤ä½¿ç”¨äº†æ­£ç¡®çš„æ¨¡å‹")

print("\n" + "="*70)
print("âœ… ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ")
print("="*70 + "\n")

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
try:
    os.unlink(config_file)
    print(f"å·²æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶: {config_file}")
except:
    pass
