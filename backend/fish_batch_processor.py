"""
æ‰¹å¤„ç†æ ¸å¿ƒæ¨¡å— - Layer 2 ä¼˜åŒ–
å°è£…æ‰¹é‡ç”Ÿæˆé€»è¾‘ï¼Œå¯åœ¨ worker è¿›ç¨‹ä¸­å¤ç”¨

ä½œè€…ï¼šClaude (ç¬¬äºŒé˜¶æ®µä¼˜åŒ–)
"""
import os
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from loguru import logger
from fish_io_pipeline import IOPipeline


class BatchProcessor:
    """
    æ‰¹å¤„ç†å™¨ - å°è£…æ‰¹é‡è¯­éŸ³ç”Ÿæˆé€»è¾‘

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ‰¹é‡ç”Ÿæˆä¸€ä¸ªè¯´è¯äººçš„æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ
    - é›†æˆæ‰¹é‡ DAC è§£ç ä¼˜åŒ–ï¼ˆLayer 2ï¼‰
    - é›†æˆå¼‚æ­¥ I/O æµæ°´çº¿ï¼ˆLayer 3ï¼‰
    - å¯åœ¨ worker è¿›ç¨‹ä¸­å¤ç”¨
    """

    def __init__(
        self,
        models: Dict,
        batch_size: int = 10,
        io_threads: int = 2
    ):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨

        Args:
            models: æ¨¡å‹å­—å…¸ï¼ŒåŒ…å«:
                - "dac": DAC è§£ç æ¨¡å‹
                - "text2semantic": Text2Semantic æ¨¡å‹
                - "decode_one_token": è§£ç å‡½æ•°
            batch_size: DAC æ‰¹é‡è§£ç å¤§å°ï¼Œé»˜è®¤ 5
            io_threads: I/O çº¿ç¨‹æ•°ï¼Œé»˜è®¤ 2
        """
        self.dac_model = models["dac"]
        self.text2semantic_model = models["text2semantic"]
        self.decode_one_token = models["decode_one_token"]
        self.batch_size = batch_size
        self.io_pipeline = IOPipeline(num_threads=io_threads)

    def batch_generate_for_speaker(
        self,
        texts: List[str],
        segment_indices: List[int],
        prompt_tokens: torch.Tensor,
        ref_text: str,
        output_dir: str,
        device: str,
        max_new_tokens: int = 4096,
        top_p: float = 0.7,
        temperature: float = 0.7,
        repetition_penalty: float = 1.2
    ) -> Dict[int, str]:
        """
        æ‰¹é‡ç”Ÿæˆä¸€ä¸ªè¯´è¯äººçš„æ‰€æœ‰éŸ³é¢‘ç‰‡æ®µ

        Args:
            texts: ç›®æ ‡æ–‡æœ¬åˆ—è¡¨
            segment_indices: ç‰‡æ®µç´¢å¼•åˆ—è¡¨
            prompt_tokens: è¯´è¯äººå‚è€ƒéŸ³é¢‘çš„ VQ codes
            ref_text: å‚è€ƒæ–‡æœ¬
            output_dir: è¾“å‡ºç›®å½•
            device: è®¡ç®—è®¾å¤‡
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            top_p: nucleus sampling å‚æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            repetition_penalty: é‡å¤æƒ©ç½šç³»æ•°

        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶å­—å…¸ {segment_index: file_path}
        """
        # å¯¼å…¥ Fish-Speech æ¨ç†æ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…è¿›ç¨‹é—´å†²çªï¼‰
        from fish_speech.models.text2semantic.inference import generate_long

        total_segments = len(texts)
        generated_files = {}

        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ {total_segments} ä¸ªç‰‡æ®µï¼ˆæ‰¹å¤§å°: {self.batch_size}ï¼‰...")

        try:
            # åˆ†æ‰¹å¤„ç†
            for batch_start in range(0, total_segments, self.batch_size):
                batch_end = min(batch_start + self.batch_size, total_segments)
                batch_texts = texts[batch_start:batch_end]
                batch_indices = segment_indices[batch_start:batch_end]
                batch_num = batch_start // self.batch_size + 1

                logger.info(
                    f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}: å¤„ç†ç‰‡æ®µ {batch_start}-{batch_end-1} "
                    f"({len(batch_texts)} ä¸ª)"
                )

                # Step 1: ç”Ÿæˆæ‰€æœ‰è¯­ä¹‰ tokensï¼ˆä»éœ€å¾ªç¯ï¼Œå— generate_long é™åˆ¶ï¼‰
                codes_list = []
                valid_indices = []

                for i, text in enumerate(batch_texts):
                    segment_idx = batch_indices[i]
                    logger.info(
                        f"  [{i+1}/{len(batch_texts)}] ç”Ÿæˆè¯­ä¹‰ tokens "
                        f"(ç‰‡æ®µ {segment_idx}): {text[:30]}..."
                    )

                    try:
                        codes = None
                        for response in generate_long(
                            model=self.text2semantic_model,
                            device=device,
                            decode_one_token=self.decode_one_token,
                            text=text,
                            prompt_text=ref_text,
                            prompt_tokens=prompt_tokens,
                            max_new_tokens=max_new_tokens,
                            top_p=top_p,
                            temperature=temperature,
                            repetition_penalty=repetition_penalty,
                            num_samples=1
                        ):
                            if response.action == "sample":
                                codes = response.codes

                        if codes is not None:
                            codes_list.append(codes)
                            valid_indices.append(segment_idx)
                        else:
                            logger.error(f"  âŒ æœªèƒ½ç”Ÿæˆè¯­ä¹‰ tokens (ç‰‡æ®µ {segment_idx})")

                    except Exception as e:
                        logger.error(f"  âŒ ç”Ÿæˆå¤±è´¥ (ç‰‡æ®µ {segment_idx}): {e}")

                if not codes_list:
                    logger.warning(f"  âš ï¸ æ‰¹æ¬¡ {batch_num} æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•è¯­ä¹‰ tokens")
                    continue

                # Step 2: æ‰¹é‡ DAC è§£ç ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
                logger.info(f"  ğŸ”„ æ‰¹é‡ DAC è§£ç  {len(codes_list)} ä¸ªæ ·æœ¬...")
                audios = self._batch_dac_decode(codes_list, device)
                logger.info(f"  âœ… æ‰¹é‡è§£ç å®Œæˆ")

                # Step 3: å¼‚æ­¥ä¿å­˜ï¼ˆLayer 3 ä¼˜åŒ–ï¼‰
                logger.info(f"  ğŸ’¾ å¼‚æ­¥ä¿å­˜ {len(audios)} ä¸ªéŸ³é¢‘æ–‡ä»¶...")
                for audio, segment_idx in zip(audios, valid_indices):
                    output_filename = os.path.join(
                        output_dir,
                        f"segment_{segment_idx:04d}.wav"
                    )
                    # å¼‚æ­¥æäº¤ä¿å­˜ä»»åŠ¡ï¼ˆä¸é˜»å¡ï¼‰
                    self.io_pipeline.async_save_audio(
                        audio=audio,
                        path=output_filename,
                        sample_rate=self.dac_model.sample_rate
                    )
                    generated_files[segment_idx] = output_filename

                logger.info(f"  âœ… æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆ")

                # æ¸…ç†æ˜¾å­˜
                del codes_list, audios
                if device == "mps":
                    torch.mps.empty_cache()
                elif torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # ç­‰å¾…æ‰€æœ‰ I/O å®Œæˆ
            logger.info(f"\nâ³ ç­‰å¾…æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶ä¿å­˜å®Œæˆ...")
            saved_files = self.io_pipeline.wait_all()
            logger.info(f"âœ… å·²ä¿å­˜ {len(saved_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
            raise

        return generated_files

    def _batch_dac_decode(
        self,
        codes_list: List[torch.Tensor],
        device: str
    ) -> List[np.ndarray]:
        """
        æ‰¹é‡ DAC è§£ç  - Layer 2 æ ¸å¿ƒä¼˜åŒ–

        åŸæ¥ï¼šé€ä¸ªè§£ç ï¼ŒN æ¬¡ GPU è°ƒç”¨
        ç°åœ¨ï¼šæ‰¹é‡è§£ç ï¼Œ1 æ¬¡ GPU è°ƒç”¨

        Args:
            codes_list: è¯­ä¹‰ codes åˆ—è¡¨
            device: è®¡ç®—è®¾å¤‡

        Returns:
            éŸ³é¢‘æ•°ç»„åˆ—è¡¨
        """
        if not codes_list:
            return []

        # å‡†å¤‡æ‰¹é‡è¾“å…¥
        batch_codes = []
        codes_lens = []

        for codes in codes_list:
            if codes.ndim == 2:
                codes = codes.unsqueeze(0)
            batch_codes.append(codes)
            codes_lens.append(codes.shape[-1])

        # Padding åˆ°ç›¸åŒé•¿åº¦
        max_len = max(codes_lens)
        padded_codes = []

        for codes in batch_codes:
            if codes.shape[-1] < max_len:
                # Padding
                pad_len = max_len - codes.shape[-1]
                codes = torch.nn.functional.pad(codes, (0, pad_len), value=0)
            padded_codes.append(codes)

        # æ‰¹é‡æ‹¼æ¥
        batch_tensor = torch.cat(padded_codes, dim=0)  # [B, C, T]
        codes_lens_tensor = torch.tensor(codes_lens, device=device, dtype=torch.long)

        # æ‰¹é‡è§£ç ï¼ˆä¸€æ¬¡ GPU è°ƒç”¨ï¼ï¼‰
        with torch.no_grad():
            batch_fake_audios, audio_lengths = self.dac_model.decode(
                batch_tensor,
                codes_lens_tensor
            )

        # åˆ†ç¦»å„ä¸ªéŸ³é¢‘ï¼Œå¹¶æ ¹æ®å®é™…é•¿åº¦è£å‰ª
        audios = []
        for i in range(len(codes_lens)):
            # è·å–å®é™…éŸ³é¢‘é•¿åº¦ï¼ˆdecode å‡½æ•°å·²ç»è®¡ç®—å¥½äº†ï¼‰
            actual_audio_len = audio_lengths[i].item()

            # æå–éŸ³é¢‘å¹¶è£å‰ªåˆ°å®é™…é•¿åº¦ï¼ˆå»é™¤ paddingï¼‰
            audio = batch_fake_audios[i, 0].float().cpu().numpy()
            audio = audio[:actual_audio_len]
            audios.append(audio)

        return audios

    def shutdown(self):
        """
        å…³é—­æ‰¹å¤„ç†å™¨ï¼Œç­‰å¾…æ‰€æœ‰ I/O å®Œæˆ
        """
        self.io_pipeline.shutdown(wait=True)

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        self.shutdown()
        return False
