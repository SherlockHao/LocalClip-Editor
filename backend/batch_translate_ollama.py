"""
åŸºäº Ollama çš„æ‰¹é‡ç¿»è¯‘è„šæœ¬
å°†ä¸­æ–‡å­—å¹•ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€
ä½¿ç”¨åŒæ­¥è°ƒç”¨ï¼Œä¼˜åŒ–æ€§èƒ½
"""
import sys
import os
import json
import time
from typing import List, Dict, Any
import requests

# å¼ºåˆ¶ UTF-8 è¾“å‡º
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')


# JSON æå–å‡½æ•°
def extract_translation_from_json(text: str, fallback: str = "") -> str:
    """ä»JSONæ ¼å¼çš„æ¨¡å‹è¾“å‡ºä¸­æå–ç¿»è¯‘ç»“æœ"""
    import re
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "tr" in data:
            result = data["tr"].strip()
            if result.lower() not in ['translation', 'tr', 'key', 'value', '']:
                return result
    except:
        pass

    # å°è¯•ä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡
    json_patterns = [
        r'\{["\']tr["\']\s*:\s*["\']([^"\']+)["\']\s*\}',
        r'\{\s*"tr"\s*:\s*"([^"]+)"\s*\}',
    ]

    for pattern in json_patterns:
        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
        if match:
            result = match.group(1).strip()
            if result and result.lower() not in ['translation', 'tr', 'key', 'value']:
                return result

    return fallback if fallback else text

# ä½¿ç”¨ 127.0.0.1 é¿å… Windows ä¸‹ localhost çš„ IPv6 è§£æå»¶è¿Ÿ
OLLAMA_API_URL = 'http://127.0.0.1:11434/v1/chat/completions'

# åˆ›å»ºä¸€ä¸ªsessionä»¥å¤ç”¨è¿æ¥
SESSION = requests.Session()
SESSION.headers.update({'Content-Type': 'application/json'})


def start_ollama_service():
    """
    è‡ªåŠ¨å¯åŠ¨ Ollama æœåŠ¡
    """
    import subprocess
    import platform

    print("ğŸš€ æ­£åœ¨å¯åŠ¨ Ollama æœåŠ¡...", flush=True)

    try:
        if platform.system() == "Windows":
            # Windows: ä½¿ç”¨ START å‘½ä»¤åœ¨æ–°çª—å£ä¸­å¯åŠ¨ ollama serve
            subprocess.Popen(
                ["cmd", "/c", "start", "ollama", "serve"],
                shell=False,
                creationflags=subprocess.CREATE_NEW_CONSOLE
            )
        else:
            # Linux/Mac: åå°å¯åŠ¨
            subprocess.Popen(
                ["ollama", "serve"],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
                start_new_session=True
            )

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        print("â³ ç­‰å¾… Ollama æœåŠ¡å¯åŠ¨...", flush=True)
        max_retries = 10
        for i in range(max_retries):
            time.sleep(2)
            try:
                response = SESSION.get("http://127.0.0.1:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    print(f"âœ… Ollama æœåŠ¡å·²å¯åŠ¨ï¼", flush=True)
                    return True
            except:
                pass
            print(f"  ç­‰å¾…ä¸­... ({i+1}/{max_retries})", flush=True)

        print("âŒ Ollama æœåŠ¡å¯åŠ¨è¶…æ—¶", flush=True)
        return False

    except Exception as e:
        print(f"âŒ å¯åŠ¨ Ollama æœåŠ¡å¤±è´¥: {e}", flush=True)
        return False


def warm_up(model: str = "qwen2.5:32b"):
    """
    çƒ­å¯åŠ¨å‡½æ•°ï¼šå‘é€ä¸€ä¸ªç©ºè¯·æ±‚ï¼Œç¡®ä¿æ¨¡å‹ä»ç¡¬ç›˜åŠ è½½åˆ°äº†æ˜¾å­˜ä¸­ã€‚
    å¦‚æœ Ollama æœªå¯åŠ¨ï¼Œä¼šè‡ªåŠ¨å¯åŠ¨æœåŠ¡ã€‚
    """
    print(f"ğŸ”¥ æ­£åœ¨è¿›è¡Œçƒ­å¯åŠ¨ (åŠ è½½æ¨¡å‹ {model} åˆ°æ˜¾å­˜)...", flush=True)
    start = time.time()

    # å…ˆå°è¯•è¿æ¥
    max_attempts = 2
    for attempt in range(max_attempts):
        try:
            # è®¾ç½® keep_alive=-1 è®©æ¨¡å‹æ°¸ä¹…ä¿æŒåœ¨æ˜¾å­˜ä¸­
            response = SESSION.post(
                OLLAMA_API_URL,
                json={
                    'model': model,
                    'messages': [{"role": "user", "content": "hi"}],
                    'max_tokens': 1,
                    'keep_alive': -1
                },
                timeout=30
            )
            response.raise_for_status()
            elapsed = time.time() - start
            print(f"âœ… çƒ­å¯åŠ¨å®Œæˆï¼åŠ è½½è€—æ—¶: {elapsed:.2f}s", flush=True)
            print(f"âœ… æ¨¡å‹å·²é”å®šåœ¨æ˜¾å­˜ä¸­ï¼ˆä¸ä¼šè‡ªåŠ¨å¸è½½ï¼‰", flush=True)
            print("-" * 60, flush=True)
            return

        except requests.exceptions.ConnectionError as e:
            if attempt == 0:
                # ç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œå°è¯•å¯åŠ¨ Ollama
                print(f"âš ï¸ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ (å°è¯• {attempt+1}/{max_attempts})", flush=True)
                if start_ollama_service():
                    # é‡æ–°è®¡æ—¶
                    start = time.time()
                    continue
                else:
                    print(f"âŒ è¿æ¥ Ollama å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¼€å¯ã€‚é”™è¯¯ä¿¡æ¯: {e}", flush=True)
                    raise
            else:
                # ç¬¬äºŒæ¬¡è¿˜æ˜¯å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                print(f"âŒ è¿æ¥ Ollama å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¼€å¯ã€‚é”™è¯¯ä¿¡æ¯: {e}", flush=True)
                raise
        except Exception as e:
            print(f"âŒ è¿æ¥ Ollama å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¼€å¯ã€‚é”™è¯¯ä¿¡æ¯: {e}", flush=True)
            raise


def translate_batch_group(
    sentences: List[str],
    target_language: str,
    task_id_prefix: str,
    model: str = "qwen2.5:32b"
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ç¿»è¯‘ä»»åŠ¡ï¼Œæä¾›ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›

    Args:
        sentences: æºæ–‡æœ¬åˆ—è¡¨ï¼ˆæœ‰é¡ºåºçš„ä¸Šä¸‹æ–‡ï¼‰
        target_language: ç›®æ ‡è¯­è¨€
        task_id_prefix: ä»»åŠ¡å‰ç¼€
        model: æ¨¡å‹åç§°
    """
    if not sentences:
        return []

    # 1. æ ¹æ®è¯­è¨€ç‰¹æ€§æ„å»ºæŒ‡ä»¤
    if 'æ—¥' in target_language or 'ja' in target_language.lower():
        lang_rule = "å…¨å‡åæ— æ±‰å­—"
    elif 'éŸ©' in target_language or 'ko' in target_language.lower():
        lang_rule = "çº¯éŸ©æ–‡æ— æ±‰å­—"
    else:
        lang_rule = "åœ°é“ç®€æ´"

    # 2. æ„å»º System Promptï¼šç²¾ç®€æœ‰åŠ›
    system_prompt = (
        f"ä½ æ˜¯é…éŸ³å­—å¹•å‹ç¼©ä¸“å®¶ã€‚ç¿»è¯‘ä¸º{target_language}ã€‚è¿™æ˜¯è¿ç»­å¯¹è¯ç‰‡æ®µï¼Œéœ€ä¿æŒ:\n"
        f"1. ä¸Šä¸‹æ–‡è¿è´¯(ä»£è¯/æŒ‡ä»£/è¯­æ°”ç»Ÿä¸€)\n"
        f"2. {lang_rule}\n"
        f"3. æç®€è¡¨è¾¾(æœ€å£è¯­çš„ç¼©ç•¥å½¢å¼/å»å†—ä½™)ï¼Œå­—æ•°æå°‘\n"
        f"4. å®å¯æ¼è¯‘ä¹Ÿä¸è¦é•¿è¯‘\n"
        f"è¿”å›JSON: {{\"translations\": [\"è¯‘æ–‡1\", \"è¯‘æ–‡2\", ...]}}"
    )

    # 3. æ„å»º User Promptï¼šæ˜ç¡®æ ‡æ³¨è¿™æ˜¯è¿ç»­å¯¹è¯
    user_content = "ã€è¿ç»­å¯¹è¯ã€‘è¯·æ ¹æ®ä¸Šä¸‹æ–‡ç¿»è¯‘:\n"
    for i, s in enumerate(sentences):
        user_content += f"{i}. {s}\n"

    try:
        start_time = time.time()

        response = SESSION.post(
            OLLAMA_API_URL,
            json={
                'model': model,
                'messages': [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                'temperature': 0.3,  # é€‚å½“æå‡ä¸€ç‚¹ç‚¹éšæœºæ€§ï¼Œæœ‰åŠ©äºä¸Šä¸‹æ–‡è¡”æ¥æ›´è‡ªç„¶
                'response_format': {"type": "json_object"},
                'stream': False,
                'keep_alive': -1
            },
            timeout=60  # æ‰¹é‡ç¿»è¯‘è€—æ—¶è¾ƒé•¿ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
        )

        response.raise_for_status()
        result_json = response.json()
        raw_content = result_json['choices'][0]['message']['content'].strip()

        # è§£æè¿”å›çš„ JSON æ•°ç»„
        parsed_data = json.loads(raw_content)
        translated_list = parsed_data.get("translations", [])

        # 4. ç»“æœæ ¡éªŒä¸ç»„è£…
        results = []
        elapsed = (time.time() - start_time) / len(sentences)  # å¹³å‡å•å¥è€—æ—¶

        for i, original_text in enumerate(sentences):
            # å¦‚æœæ¨¡å‹æ¼è¯‘äº†ï¼ˆè™½ç„¶ json_object æ¨¡å¼ä¸‹æ¦‚ç‡ä½ï¼‰ï¼Œåˆ™å…œåº•è¿”å›åŸæ–‡
            tr_text = translated_list[i] if i < len(translated_list) else original_text

            results.append({
                "task_id": f"{task_id_prefix}_{i}",
                "source": original_text,
                "translation": tr_text,
                "success": i < len(translated_list),
                "elapsed": elapsed
            })

        return results

    except Exception as e:
        print(f"[æ‰¹é‡ç¿»è¯‘é”™è¯¯] {task_id_prefix}: {e}", flush=True)
        # å¤±è´¥å…¨é‡å…œåº•
        return [{
            "task_id": f"{task_id_prefix}_{i}",
            "source": s,
            "translation": s,
            "success": False,
            "error": str(e)
        } for i, s in enumerate(sentences)]


def translate_single(
    sentence: str,
    target_language: str,
    task_id: str,
    model: str = "qwen2.5:32b"
) -> Dict[str, Any]:
    """
    å•ä¸ªç¿»è¯‘ä»»åŠ¡ï¼ˆåŒæ­¥ï¼‰- ä¿ç•™ç”¨äºå•å¥ç¿»è¯‘

    Args:
        sentence: æºæ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰
        target_language: ç›®æ ‡è¯­è¨€ï¼ˆä¸­æ–‡åç§°ï¼Œå¦‚"è‹±è¯­"ã€"æ—¥è¯­"ç­‰ï¼‰
        task_id: ä»»åŠ¡ID
        model: æ¨¡å‹åç§°

    Returns:
        dict: ç¿»è¯‘ç»“æœ
    """
    # ç›´æ¥è°ƒç”¨æ‰¹é‡ç¿»è¯‘ï¼Œä¼ å…¥å•ä¸ªå¥å­
    results = translate_batch_group([sentence], target_language, task_id, model)
    if results:
        return results[0]
    else:
        return {
            "task_id": task_id,
            "source": sentence,
            "translation": sentence,
            "success": False,
            "error": "Translation failed"
        }


def parse_time_to_seconds(time_str: str) -> float:
    """å°† SRT æ—¶é—´æˆ³è½¬æ¢ä¸ºç§’æ•°"""
    try:
        # æ ¼å¼: 00:00:01,500
        parts = time_str.replace(',', '.').split(':')
        hours = float(parts[0])
        minutes = float(parts[1])
        seconds = float(parts[2])
        return hours * 3600 + minutes * 60 + seconds
    except:
        return 0.0


def group_tasks_by_time(tasks: List[Dict], max_gap_seconds: float = 1.0, max_group_size: int = 5) -> List[List[Dict]]:
    """
    æ ¹æ®æ—¶é—´é—´éš”å¯¹ä»»åŠ¡è¿›è¡Œåˆ†ç»„

    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼ˆéœ€åŒ…å« start_time, end_timeï¼‰
        max_gap_seconds: æœ€å¤§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
        max_group_size: æ¯ç»„æœ€å¤§ä»»åŠ¡æ•°

    Returns:
        åˆ†ç»„åçš„ä»»åŠ¡åˆ—è¡¨
    """
    if not tasks:
        return []

    # æŒ‰ index æ’åºç¡®ä¿é¡ºåºæ­£ç¡®
    sorted_tasks = sorted(tasks, key=lambda t: t.get('index', 0))

    groups = []
    current_group = []

    for task in sorted_tasks:
        if not current_group:
            # ç¬¬ä¸€ä¸ªä»»åŠ¡ï¼Œç›´æ¥åŠ å…¥
            current_group.append(task)
        else:
            # æ£€æŸ¥æ—¶é—´é—´éš”
            prev_task = current_group[-1]
            prev_end = parse_time_to_seconds(prev_task.get('end_time', '00:00:00,000'))
            curr_start = parse_time_to_seconds(task.get('start_time', '00:00:00,000'))
            gap = curr_start - prev_end

            # å¦‚æœé—´éš”å°äºé˜ˆå€¼ä¸”ç»„å†…ä»»åŠ¡æ•°æœªè¾¾ä¸Šé™ï¼ŒåŠ å…¥å½“å‰ç»„
            if gap <= max_gap_seconds and len(current_group) < max_group_size:
                current_group.append(task)
            else:
                # å¦åˆ™ï¼Œå¼€å§‹æ–°ç»„
                groups.append(current_group)
                current_group = [task]

    # æ·»åŠ æœ€åä¸€ç»„
    if current_group:
        groups.append(current_group)

    return groups


def batch_translate(
    tasks: List[Dict[str, str]],
    model: str = "qwen2.5:32b"
) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡ç¿»è¯‘ä»»åŠ¡ï¼ˆæ”¯æŒä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆ†ç»„ï¼‰

    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«:
            - task_id: ä»»åŠ¡ID
            - source: æºæ–‡æœ¬ï¼ˆä¸­æ–‡ï¼‰
            - target_language: ç›®æ ‡è¯­è¨€
            - start_time: å¼€å§‹æ—¶é—´ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†ç»„ï¼‰
            - end_time: ç»“æŸæ—¶é—´ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†ç»„ï¼‰
            - index: ç´¢å¼•ï¼ˆç”¨äºæ’åºï¼‰

        model: ä½¿ç”¨çš„æ¨¡å‹åç§°

    Returns:
        List[Dict]: ç¿»è¯‘ç»“æœåˆ—è¡¨
    """
    # 1. çƒ­å¯åŠ¨ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹ Ollama æ˜¯å¦è¿è¡Œï¼‰
    try:
        warm_up(model=model)
    except Exception as e:
        print(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡å™¨: {e}", flush=True)
        print("è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨ï¼ˆè¿è¡Œ 'ollama serve'ï¼‰", flush=True)
        return []

    total = len(tasks)
    print(f"\n[ç¿»è¯‘] å¼€å§‹ç¿»è¯‘ {total} æ¡å­—å¹•...\n", flush=True)

    # 2. æ ¹æ®æ—¶é—´é—´éš”åˆ†ç»„
    task_groups = group_tasks_by_time(tasks, max_gap_seconds=1.0, max_group_size=5)
    print(f"[ç¿»è¯‘] åˆ†ç»„ç­–ç•¥: {len(task_groups)} ç»„ï¼ˆæ—¶é—´é—´éš”â‰¤1ç§’ï¼Œæ¯ç»„â‰¤5å¥ï¼‰\n", flush=True)

    # è®°å½•æ€»æ—¶é•¿
    batch_start_time = time.time()
    results = []
    processed_count = 0

    # 3. æŒ‰ç»„ç¿»è¯‘
    for group_idx, group in enumerate(task_groups, 1):
        # æå–è¯¥ç»„çš„æ‰€æœ‰å¥å­
        sentences = [t["source"] for t in group]
        target_language = group[0]["target_language"]  # åŒä¸€ç»„çš„ç›®æ ‡è¯­è¨€åº”è¯¥ç›¸åŒ
        group_prefix = f"group-{group_idx}"

        # æ‰¹é‡ç¿»è¯‘è¯¥ç»„
        group_results = translate_batch_group(sentences, target_language, group_prefix, model)

        # å°†ç»“æœæ˜ å°„å›åŸå§‹ task_id
        for i, task in enumerate(group):
            if i < len(group_results):
                # ä¿ç•™åŸå§‹ task_id
                group_results[i]["task_id"] = task["task_id"]
                results.append(group_results[i])

                # å®æ—¶è¾“å‡ºè¿›åº¦
                processed_count += 1
                status = "âœ“" if group_results[i]["success"] else "âœ—"
                elapsed = group_results[i].get("elapsed", 0)
                source = group_results[i]["source"][:20] + "..." if len(group_results[i]["source"]) > 20 else group_results[i]["source"]
                translation = group_results[i]["translation"][:30] + "..." if len(group_results[i]["translation"]) > 30 else group_results[i]["translation"]

                print(
                    f"[{processed_count}/{total}] {status} {group_results[i]['task_id']}: {source} -> {translation} "
                    f"({elapsed:.2f}s)",
                    flush=True
                )

    # è®¡ç®—æ€»æ—¶é•¿
    total_elapsed = time.time() - batch_start_time
    avg_elapsed = total_elapsed / total if total > 0 else 0

    print(f"\n[ç¿»è¯‘] âœ“ å®Œæˆæ‰€æœ‰ç¿»è¯‘", flush=True)
    print(f"[ç¿»è¯‘] æ€»è€—æ—¶: {total_elapsed:.2f}ç§’ | å¹³å‡: {avg_elapsed:.2f}ç§’/æ¡\n", flush=True)

    # å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾GPU
    try:
        print(f"[ç¿»è¯‘] æ­£åœ¨å¸è½½æ¨¡å‹ {model}ï¼Œé‡Šæ”¾GPU...", flush=True)
        unload_response = SESSION.post(
            'http://127.0.0.1:11434/api/generate',
            json={
                'model': model,
                'keep_alive': 0
            },
            timeout=10
        )
        if unload_response.status_code == 200:
            print(f"[ç¿»è¯‘] âœ“ æ¨¡å‹å·²å¸è½½ï¼ŒGPUå·²é‡Šæ”¾", flush=True)
    except Exception as e:
        print(f"[ç¿»è¯‘] âš  å¸è½½æ¨¡å‹å¤±è´¥: {e}", flush=True)

    return results


def main(config_path: str):
    """
    ä¸»å‡½æ•°

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæ ¼å¼ï¼‰
    """
    # è¯»å–é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    tasks = config.get("tasks", [])
    model = config.get("model", "qwen2.5:32b")

    if not tasks:
        print("âŒ æ²¡æœ‰ç¿»è¯‘ä»»åŠ¡", flush=True)
        return

    # æ‰§è¡Œæ‰¹é‡ç¿»è¯‘
    results = batch_translate(tasks, model=model)

    # è¾“å‡ºç»“æœåˆ°æ ‡å‡†è¾“å‡ºï¼ˆJSONæ ¼å¼ï¼‰
    print("\n" + "="*60, flush=True)
    print("ç¿»è¯‘ç»“æœï¼ˆJSONï¼‰:", flush=True)
    print("="*60, flush=True)
    print(json.dumps(results, ensure_ascii=False, indent=2), flush=True)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python batch_translate_ollama.py <config.json>")
        sys.exit(1)

    config_path = sys.argv[1]

    if not os.path.exists(config_path):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)

    # è¿è¡Œä¸»å‡½æ•°
    main(config_path)
