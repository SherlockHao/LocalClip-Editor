"""
è·¨ç¯å¢ƒå¹¶è¡Œå…‹éš†å¼•æ“
ä½¿ç”¨ subprocess å¯åŠ¨ fish-speech ç¯å¢ƒçš„ worker è¿›ç¨‹

è¿™ä¸ªæ–¹æ¡ˆè§£å†³äº†ç¯å¢ƒä¾èµ–å†²çªï¼š
- ui ç¯å¢ƒï¼šè¿è¡Œ FastAPI å’Œåè°ƒå™¨
- fish-speech ç¯å¢ƒï¼šè¿è¡Œ worker è¿›ç¨‹ï¼ˆæœ‰å®Œæ•´ä¾èµ–ï¼‰

ä½œè€…ï¼šClaude
"""
import os
import sys
import json
import platform
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, List
from loguru import logger


# è·å–ç¯å¢ƒé…ç½®
if platform.system() == "Windows":
    FISH_SPEECH_DIR = os.environ.get("FISH_SPEECH_DIR", r"d:/ai_editing\fish-speech-win")
    FISH_PYTHON = os.environ.get("FISH_SPEECH_PYTHON", r"C:\Users\7\miniconda3\envs\fish-speech\python.exe")
else:
    FISH_SPEECH_DIR = os.environ.get("FISH_SPEECH_DIR", "/Users/yiya_workstation/Documents/ai_editing/fish-speech")
    FISH_PYTHON = os.environ.get("FISH_SPEECH_PYTHON", "/Users/yiya_workstation/miniconda3/envs/fish-speech/bin/python")

CHECKPOINT_DIR = os.path.join(FISH_SPEECH_DIR, "checkpoints", "openaudio-s1-mini")


class SubprocessParallelCloner:
    """
    ä½¿ç”¨ subprocess çš„å¹¶è¡Œå…‹éš†å™¨

    ä¼˜åŠ¿ï¼š
    - è·¨è¯´è¯äººçœŸæ­£å¹¶è¡Œ
    - æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼ˆæ¯ä¸ª workerï¼‰
    - é¿å…ç¯å¢ƒä¾èµ–å†²çª
    """

    def __init__(
        self,
        num_workers: int = 2,
        checkpoint_path: str = CHECKPOINT_DIR,
        batch_size: int = 10
    ):
        self.num_workers = num_workers
        self.checkpoint_path = checkpoint_path
        self.batch_size = batch_size
        self.fish_python = FISH_PYTHON
        self.fish_speech_dir = FISH_SPEECH_DIR

    def batch_encode_speakers(
        self,
        speaker_references: Dict[int, Dict],
        output_dir: str
    ) -> Dict[int, str]:
        """
        æ‰¹é‡ç¼–ç è¯´è¯äººå‚è€ƒéŸ³é¢‘
        ä½¿ç”¨ FishBatchCloner çš„å®ç°ï¼ˆç¼–ç é€Ÿåº¦å·²è¶³å¤Ÿå¿«ï¼Œä¸éœ€è¦å¹¶è¡Œï¼‰

        Args:
            speaker_references: è¯´è¯äººå‚è€ƒä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            è¯´è¯äººç¼–ç æ–‡ä»¶æ˜ å°„ {speaker_id: npy_file_path}
        """
        from fish_batch_cloner import FishBatchCloner

        logger.info("ğŸ”„ ä½¿ç”¨æ‰¹é‡ç¼–ç æ¨¡å¼ï¼ˆé€šè¿‡ fish-speech ç¯å¢ƒï¼‰")
        cloner = FishBatchCloner(
            fish_speech_dir=self.fish_speech_dir,
            checkpoint_dir=self.checkpoint_path
        )
        return cloner.batch_encode_speakers(speaker_references, output_dir)

    def batch_generate_audio(
        self,
        tasks: List[Dict],
        speaker_npy_files: Dict[int, str],
        speaker_references: Dict[int, Dict],
        output_dir: str,
        script_dir: str = None  # å…¼å®¹å‚æ•°ï¼Œä¸ä½¿ç”¨ï¼ˆsubprocessæ¨¡å¼ä¸éœ€è¦è„šæœ¬ç›®å½•ï¼‰
    ) -> Dict[str, str]:
        """
        å¹¶è¡Œç”ŸæˆéŸ³é¢‘

        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            speaker_npy_files: è¯´è¯äººç¼–ç æ–‡ä»¶æ˜ å°„
            speaker_references: è¯´è¯äººå‚è€ƒä¿¡æ¯
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶æ˜ å°„
        """
        # æŒ‰è¯´è¯äººåˆ†ç»„ä»»åŠ¡
        tasks_by_speaker = self._group_by_speaker(tasks)

        logger.info(f"\nğŸ“Š ä»»åŠ¡ç»Ÿè®¡:")
        for speaker_id, speaker_tasks in tasks_by_speaker.items():
            logger.info(f"  è¯´è¯äºº {speaker_id}: {len(speaker_tasks)} ä¸ªç‰‡æ®µ")

        # åˆ›å»ºä¸´æ—¶ä»»åŠ¡æ–‡ä»¶
        task_files = []
        for speaker_id, speaker_tasks in tasks_by_speaker.items():
            task_file = self._create_task_file(
                speaker_id,
                speaker_tasks,
                speaker_npy_files[speaker_id],
                speaker_references[speaker_id],
                output_dir
            )
            task_files.append((speaker_id, task_file))

        # å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯´è¯äººï¼ˆä½¿ç”¨ worker æ± ï¼‰
        logger.info(f"\nğŸš€ ä½¿ç”¨ {self.num_workers} ä¸ªå¹¶è¡Œ worker å¤„ç† {len(task_files)} ä¸ªè¯´è¯äºº...")

        all_generated_files = {}
        active_processes = []
        task_queue = list(task_files)
        completed = 0

        # å¯åŠ¨åˆå§‹çš„ worker æ‰¹æ¬¡
        while len(active_processes) < self.num_workers and task_queue:
            speaker_id, task_file = task_queue.pop(0)
            proc = self._start_worker(speaker_id, task_file)
            active_processes.append((speaker_id, task_file, proc))

        # å¤„ç†æ‰€æœ‰ä»»åŠ¡
        while active_processes:
            # æ£€æŸ¥å·²å®Œæˆçš„è¿›ç¨‹
            for i in range(len(active_processes) - 1, -1, -1):
                speaker_id, task_file, proc = active_processes[i]

                # éé˜»å¡æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å®Œæˆ
                retcode = proc.poll()
                if retcode is not None:
                    # è¿›ç¨‹å·²å®Œæˆ
                    completed += 1
                    active_processes.pop(i)

                    try:
                        stdout, stderr = proc.communicate(timeout=1)
                        stderr_text = stderr.decode('utf-8', errors='ignore')

                        if retcode == 0:
                            # è§£æ JSON è¾“å‡º
                            stdout_text = stdout.decode('utf-8', errors='ignore')
                            # åªè§£ææœ€åä¸€è¡Œï¼ˆJSON è¾“å‡ºï¼‰
                            json_line = stdout_text.strip().split('\n')[-1] if stdout_text.strip() else "{}"
                            result = json.loads(json_line)
                            all_generated_files.update(result)
                            logger.info(f"âœ… è¯´è¯äºº {speaker_id} å®Œæˆ ({completed}/{len(task_files)}): {len(result)} ä¸ªæ–‡ä»¶")

                            # æ˜¾ç¤º worker æ—¥å¿—
                            if stderr_text.strip():
                                logger.debug(f"Worker {speaker_id} æ—¥å¿—:\n{stderr_text}")
                        else:
                            logger.error(f"âŒ è¯´è¯äºº {speaker_id} å¤±è´¥ (è¿”å›ç : {retcode})")
                            logger.error(f"Worker stderr:\n{stderr_text}")
                    except json.JSONDecodeError as e:
                        logger.error(f"âŒ è¯´è¯äºº {speaker_id} JSON è§£æå¤±è´¥: {e}")
                        logger.error(f"Worker output: {stdout.decode('utf-8', errors='ignore')}")
                    except Exception as e:
                        logger.error(f"âŒ è¯´è¯äºº {speaker_id} é”™è¯¯: {e}")

                    # å¯åŠ¨æ–°çš„ workerï¼ˆå¦‚æœè¿˜æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼‰
                    if task_queue:
                        next_speaker_id, next_task_file = task_queue.pop(0)
                        next_proc = self._start_worker(next_speaker_id, next_task_file)
                        active_processes.append((next_speaker_id, next_task_file, next_proc))

            # çŸ­æš‚ä¼‘çœ é¿å…å¿™ç­‰å¾…
            import time
            time.sleep(0.1)

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for _, task_file in task_files:
            try:
                os.remove(task_file)
            except:
                pass

        logger.info(f"\nğŸ‰ å¹¶è¡Œç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(all_generated_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        return all_generated_files

    def _group_by_speaker(self, tasks: List[Dict]) -> Dict[int, List[Dict]]:
        """æŒ‰è¯´è¯äººåˆ†ç»„ä»»åŠ¡"""
        tasks_by_speaker = {}
        for task in tasks:
            speaker_id = task["speaker_id"]
            if speaker_id not in tasks_by_speaker:
                tasks_by_speaker[speaker_id] = []
            tasks_by_speaker[speaker_id].append(task)
        return tasks_by_speaker

    def _create_task_file(
        self,
        speaker_id: int,
        tasks: List[Dict],
        npy_file: str,
        reference: Dict,
        output_dir: str
    ) -> str:
        """åˆ›å»ºä»»åŠ¡é…ç½®æ–‡ä»¶"""
        # ç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½æ˜¯ç»å¯¹è·¯å¾„ï¼ˆworker è¿›ç¨‹å·¥ä½œç›®å½•æ˜¯ fish-speech-winï¼‰
        npy_file_abs = os.path.abspath(npy_file)
        output_dir_abs = os.path.abspath(output_dir)

        task_data = {
            "speaker_id": speaker_id,
            "tasks": tasks,
            "npy_file": npy_file_abs,
            "reference": reference,
            "output_dir": output_dir_abs,
            "checkpoint_path": self.checkpoint_path,
            "batch_size": self.batch_size,
            "fish_speech_dir": self.fish_speech_dir
        }

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        fd, path = tempfile.mkstemp(suffix='.json', prefix=f'speaker_{speaker_id}_')
        with os.fdopen(fd, 'w', encoding='utf-8') as f:
            json.dump(task_data, f, ensure_ascii=False, indent=2)

        return path

    def _start_worker(self, speaker_id: int, task_file: str) -> subprocess.Popen:
        """å¯åŠ¨ worker è¿›ç¨‹"""
        # åˆ›å»º worker è„šæœ¬
        worker_script = self._get_worker_script_path()

        # å¯åŠ¨è¿›ç¨‹
        cmd = [self.fish_python, worker_script, task_file]

        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=self.fish_speech_dir
        )

        logger.info(f"  å¯åŠ¨ worker for è¯´è¯äºº {speaker_id} (PID: {proc.pid})")
        return proc

    def _get_worker_script_path(self) -> str:
        """è·å– worker è„šæœ¬è·¯å¾„"""
        backend_dir = Path(__file__).parent
        return str(backend_dir / "fish_subprocess_worker.py")
