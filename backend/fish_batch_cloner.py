"""
Fish-Speech æ‰¹é‡è¯­éŸ³å…‹éš†æ¨¡å—
ä½¿ç”¨ fish-speech/batch_inference.py çš„æ‰¹é‡æ¨ç†èƒ½åŠ›
"""
import os
import json
import subprocess
import shutil
from pathlib import Path

# fish-speech ä»“åº“è·¯å¾„
FISH_SPEECH_DIR = "/Users/yiya_workstation/Documents/ai_editing/fish-speech"
CHECKPOINT_DIR = os.path.join(FISH_SPEECH_DIR, "checkpoints/openaudio-s1-mini")
FISH_AUDIO_PYTHON = "/Users/yiya_workstation/miniconda3/envs/fish-speech/bin/python"


class FishBatchCloner:
    """Fish-Speech æ‰¹é‡è¯­éŸ³å…‹éš†å™¨"""

    def __init__(self, fish_speech_dir=FISH_SPEECH_DIR, checkpoint_dir=CHECKPOINT_DIR):
        self.fish_speech_dir = fish_speech_dir
        self.checkpoint_dir = checkpoint_dir
        self.python_executable = FISH_AUDIO_PYTHON

    def batch_encode_speakers(self, speaker_references, output_dir):
        """
        æ‰¹é‡ç¼–ç æ‰€æœ‰è¯´è¯äººçš„å‚è€ƒéŸ³é¢‘ä¸º VQ codes

        Args:
            speaker_references: dict, {speaker_id: {"reference_audio": path, "reference_text": text, ...}}
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            dict: {speaker_id: npy_file_path}
        """
        print(f"\nğŸŸ [æ‰¹é‡ç¼–ç ] å¼€å§‹æ‰¹é‡ç¼–ç  {len(speaker_references)} ä¸ªè¯´è¯äººçš„å‚è€ƒéŸ³é¢‘...")

        os.makedirs(output_dir, exist_ok=True)

        # å‡†å¤‡æ‰¹é‡ç¼–ç è„šæœ¬
        batch_script = self._create_batch_encode_script(speaker_references, output_dir)
        script_path = os.path.abspath(os.path.join(output_dir, "batch_encode.py"))

        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(batch_script)

        # æ‰§è¡Œæ‰¹é‡ç¼–ç è„šæœ¬
        cmd = [self.python_executable, script_path]

        env = os.environ.copy()
        env["PYTHONPATH"] = self.fish_speech_dir + (f":{env.get('PYTHONPATH', '')}" if env.get('PYTHONPATH') else '')

        try:
            print(f"æ‰§è¡Œæ‰¹é‡ç¼–ç è„šæœ¬: {script_path}")
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True,
                cwd=self.fish_speech_dir,
                env=env
            )
            print(result.stdout)

            # è¯»å–ç”Ÿæˆçš„.npyæ–‡ä»¶è·¯å¾„
            speaker_npy_files = {}
            for speaker_id in speaker_references.keys():
                npy_path = os.path.join(output_dir, f"speaker_{speaker_id}_codes.npy")
                if os.path.exists(npy_path):
                    speaker_npy_files[speaker_id] = npy_path
                    print(f"âœ… è¯´è¯äºº {speaker_id} ç¼–ç å®Œæˆ: {npy_path}")
                else:
                    print(f"âŒ è¯´è¯äºº {speaker_id} ç¼–ç å¤±è´¥: æœªæ‰¾åˆ° {npy_path}")

            return speaker_npy_files

        except subprocess.CalledProcessError as e:
            print(f"âŒ æ‰¹é‡ç¼–ç å¤±è´¥: {e.stderr}")
            raise

    def batch_generate_audio(self, tasks, speaker_npy_files, speaker_references, output_dir, script_dir=None):
        """
        æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è¯­éŸ³ç‰‡æ®µ

        Args:
            tasks: list of dict, [{"speaker_id": int, "target_text": str, "segment_index": int}, ...]
            speaker_npy_files: dict, {speaker_id: npy_file_path}
            speaker_references: dict, {speaker_id: {"reference_text": text, ...}}
            output_dir: è¾“å‡ºç›®å½•
            script_dir: è„šæœ¬ä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨output_dirï¼‰

        Returns:
            dict: {segment_index: audio_file_path}
        """
        print(f"\nğŸŸ [æ‰¹é‡ç”Ÿæˆ] å¼€å§‹æ‰¹é‡ç”Ÿæˆ {len(tasks)} ä¸ªè¯­éŸ³ç‰‡æ®µ...")

        os.makedirs(output_dir, exist_ok=True)

        # å¦‚æœæœªæŒ‡å®šè„šæœ¬ç›®å½•ï¼Œä½¿ç”¨output_dir
        if script_dir is None:
            script_dir = output_dir
        else:
            os.makedirs(script_dir, exist_ok=True)

        # æŒ‰è¯´è¯äººåˆ†ç»„ä»»åŠ¡ï¼Œä»¥æé«˜æ‰¹é‡å¤„ç†æ•ˆç‡
        tasks_by_speaker = {}
        for task in tasks:
            speaker_id = task['speaker_id']
            if speaker_id not in tasks_by_speaker:
                tasks_by_speaker[speaker_id] = []
            tasks_by_speaker[speaker_id].append(task)

        generated_audio_files = {}

        # å¯¹æ¯ä¸ªè¯´è¯äººæ‰¹é‡ç”Ÿæˆå…¶æ‰€æœ‰ç‰‡æ®µ
        for speaker_id, speaker_tasks in tasks_by_speaker.items():
            if speaker_id not in speaker_npy_files:
                print(f"âš ï¸ è¯´è¯äºº {speaker_id} æ²¡æœ‰.npyæ–‡ä»¶ï¼Œè·³è¿‡")
                continue

            print(f"\nå¤„ç†è¯´è¯äºº {speaker_id} çš„ {len(speaker_tasks)} ä¸ªç‰‡æ®µ...")

            # åˆ›å»ºè¯¥è¯´è¯äººçš„æ‰¹é‡ç”Ÿæˆè„šæœ¬
            batch_script = self._create_batch_generate_script(
                speaker_id,
                speaker_tasks,
                speaker_npy_files[speaker_id],
                speaker_references[speaker_id]["reference_text"],
                output_dir
            )

            # è„šæœ¬ä¿å­˜åˆ°script_dirï¼Œé¿å…è§¦å‘uvicorn reload
            script_path = os.path.abspath(os.path.join(script_dir, f"batch_generate_speaker_{speaker_id}.py"))
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(batch_script)

            # æ‰§è¡Œæ‰¹é‡ç”Ÿæˆè„šæœ¬
            cmd = [self.python_executable, script_path]

            env = os.environ.copy()
            env["PYTHONPATH"] = self.fish_speech_dir + (f":{env.get('PYTHONPATH', '')}" if env.get('PYTHONPATH') else '')

            try:
                print(f"æ‰§è¡Œæ‰¹é‡ç”Ÿæˆè„šæœ¬: {script_path}")
                result = subprocess.run(
                    cmd,
                    check=True,
                    capture_output=True,
                    text=True,
                    cwd=self.fish_speech_dir,
                    env=env,
                    timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                )
                print(result.stdout)

                # æ”¶é›†ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶
                for task in speaker_tasks:
                    segment_index = task['segment_index']
                    audio_path = os.path.join(output_dir, f"segment_{segment_index:04d}.wav")
                    if os.path.exists(audio_path):
                        generated_audio_files[segment_index] = audio_path
                        print(f"âœ… ç‰‡æ®µ {segment_index} ç”Ÿæˆå®Œæˆ")
                    else:
                        print(f"âŒ ç‰‡æ®µ {segment_index} ç”Ÿæˆå¤±è´¥")

            except subprocess.CalledProcessError as e:
                print(f"âŒ è¯´è¯äºº {speaker_id} æ‰¹é‡ç”Ÿæˆå¤±è´¥: {e.stderr}")
                continue
            except subprocess.TimeoutExpired:
                print(f"âŒ è¯´è¯äºº {speaker_id} æ‰¹é‡ç”Ÿæˆè¶…æ—¶")
                continue

        return generated_audio_files

    def _create_batch_encode_script(self, speaker_references, output_dir):
        """åˆ›å»ºæ‰¹é‡ç¼–ç è„šæœ¬"""
        # å‡†å¤‡å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬åˆ—è¡¨
        ref_audio_list = []
        ref_text_list = []
        speaker_ids = []

        for speaker_id, ref_data in speaker_references.items():
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            audio_path = os.path.abspath(ref_data["reference_audio"])
            ref_audio_list.append(audio_path)
            ref_text_list.append(ref_data["reference_text"])
            speaker_ids.append(speaker_id)

        # ç¡®ä¿è¾“å‡ºç›®å½•ä¹Ÿæ˜¯ç»å¯¹è·¯å¾„
        output_dir = os.path.abspath(output_dir)

        script = f'''#!/usr/bin/env python3
import os
import sys
import torch
import numpy as np
import torchaudio
from loguru import logger

# å¯¼å…¥ Fish Speech çš„æ¨ç†æ¨¡å—
from fish_speech.models.dac.inference import load_model as load_dac_model

# é…ç½®
CHECKPOINT_PATH = "{self.checkpoint_dir}"
OUTPUT_DIR = "{output_dir}"

REF_AUDIO_PATH_LIST = {json.dumps(ref_audio_list)}
REF_TEXT_LIST = {json.dumps(ref_text_list)}
SPEAKER_IDS = {json.dumps(speaker_ids)}

def get_device():
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"

def main():
    device = get_device()
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {{device}}")

    # åŠ è½½ DAC æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½ DAC æ¨¡å‹...")
    dac_model = load_dac_model(
        config_name="modded_dac_vq",
        checkpoint_path=os.path.join(CHECKPOINT_PATH, "codec.pth"),
        device=device
    )
    logger.info("âœ… DAC æ¨¡å‹åŠ è½½å®Œæˆ")

    # æ‰¹é‡ç¼–ç 
    for i, (speaker_id, ref_audio, ref_text) in enumerate(zip(SPEAKER_IDS, REF_AUDIO_PATH_LIST, REF_TEXT_LIST)):
        logger.info(f"\\næ­£åœ¨ç¼–ç è¯´è¯äºº {{speaker_id}}: {{ref_audio}}")

        try:
            # åŠ è½½éŸ³é¢‘
            audio, sr = torchaudio.load(str(ref_audio))

            # è½¬æ¢ä¸ºå•å£°é“
            if audio.shape[0] > 1:
                audio = audio.mean(0, keepdim=True)

            # é‡é‡‡æ ·
            audio = torchaudio.functional.resample(audio, sr, dac_model.sample_rate)

            # ç¼–ç ä¸º VQ codes
            audios = audio[None].to(device)
            audio_lengths = torch.tensor([audios.shape[2]], device=device, dtype=torch.long)

            with torch.no_grad():
                indices, _ = dac_model.encode(audios, audio_lengths)

            if indices.ndim == 3:
                indices = indices[0]

            # ä¿å­˜ .npy æ–‡ä»¶
            npy_filename = os.path.join(OUTPUT_DIR, f"speaker_{{speaker_id}}_codes.npy")
            np.save(npy_filename, indices.cpu().numpy())
            logger.info(f"âœ… å·²ä¿å­˜: {{npy_filename}}")

            # æ¸…ç†æ˜¾å­˜
            del audio, audios, indices
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            logger.error(f"âŒ ç¼–ç è¯´è¯äºº {{speaker_id}} å¤±è´¥: {{e}}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
'''
        return script

    def _create_batch_generate_script(self, speaker_id, tasks, npy_file, ref_text, output_dir):
        """åˆ›å»ºæ‰¹é‡ç”Ÿæˆè„šæœ¬"""
        # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨å’Œè¾“å‡ºæ–‡ä»¶ååˆ—è¡¨
        text_list = [task['target_text'] for task in tasks]
        segment_indices = [task['segment_index'] for task in tasks]

        # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        npy_file = os.path.abspath(npy_file)
        output_dir = os.path.abspath(output_dir)

        script = f'''#!/usr/bin/env python3
import os
import sys
import torch
import numpy as np
import soundfile as sf
from loguru import logger

# å¯¼å…¥ Fish Speech çš„æ¨ç†æ¨¡å—
from fish_speech.models.text2semantic.inference import init_model, generate_long
from fish_speech.models.dac.inference import load_model as load_dac_model

# é…ç½®
CHECKPOINT_PATH = "{self.checkpoint_dir}"
OUTPUT_DIR = "{output_dir}"
PROMPT_TOKENS_PATH = "{npy_file}"
REF_TEXT = {json.dumps(ref_text)}
TEXT_LIST = {json.dumps(text_list)}
SEGMENT_INDICES = {json.dumps(segment_indices)}

# ç”Ÿæˆå‚æ•°
MAX_NEW_TOKENS = 1024
TOP_P = 0.7
TEMPERATURE = 0.7
REPETITION_PENALTY = 1.2

def get_device():
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"

def main():
    device = get_device()
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {{device}}")

    # è®¾ç½®ç²¾åº¦
    if device == "cuda":
        precision = torch.bfloat16
    else:
        precision = torch.float32

    # åŠ è½½ DAC æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½ DAC æ¨¡å‹...")
    dac_model = load_dac_model(
        config_name="modded_dac_vq",
        checkpoint_path=os.path.join(CHECKPOINT_PATH, "codec.pth"),
        device=device
    )
    logger.info("âœ… DAC æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½ Text2Semantic æ¨¡å‹
    logger.info("æ­£åœ¨åŠ è½½ Text2Semantic æ¨¡å‹...")
    llama_model, decode_one_token = init_model(
        checkpoint_path=CHECKPOINT_PATH,
        device=device,
        precision=precision,
        compile=False
    )
    logger.info("âœ… Text2Semantic æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½ Prompt Tokens
    logger.info(f"åŠ è½½ Prompt Tokens: {{PROMPT_TOKENS_PATH}}")
    prompt_tokens = np.load(PROMPT_TOKENS_PATH)
    prompt_tokens = torch.from_numpy(prompt_tokens).to(device).long()
    if prompt_tokens.ndim == 3:
        prompt_tokens = prompt_tokens[0]
    logger.info(f"âœ… Prompt Tokens åŠ è½½å®Œæˆ")

    # æ‰¹é‡ç”Ÿæˆ
    logger.info(f"\\nå¼€å§‹æ‰¹é‡ç”Ÿæˆ {{len(TEXT_LIST)}} ä¸ªç‰‡æ®µ...")

    for i, (text, segment_idx) in enumerate(zip(TEXT_LIST, SEGMENT_INDICES)):
        logger.info(f"\\n[{{i+1}}/{{len(TEXT_LIST)}}] ç”Ÿæˆç‰‡æ®µ {{segment_idx}}: {{text[:50]}}...")

        try:
            # ç”Ÿæˆè¯­ä¹‰ tokens
            codes = None
            for response in generate_long(
                model=llama_model,
                device=device,
                decode_one_token=decode_one_token,
                text=text,
                prompt_text=REF_TEXT,
                prompt_tokens=prompt_tokens,
                max_new_tokens=MAX_NEW_TOKENS,
                top_p=TOP_P,
                temperature=TEMPERATURE,
                repetition_penalty=REPETITION_PENALTY,
                num_samples=1
            ):
                if response.action == "sample":
                    codes = response.codes

            if codes is None:
                logger.error(f"âŒ æœªèƒ½ç”Ÿæˆè¯­ä¹‰ tokens")
                continue

            # è§£ç ä¸ºéŸ³é¢‘
            if codes.ndim == 2:
                codes = codes.unsqueeze(0)

            codes_lens = torch.tensor([codes.shape[-1]], device=device, dtype=torch.long)

            with torch.no_grad():
                fake_audios, _ = dac_model.decode(codes, codes_lens)

            # ä¿å­˜éŸ³é¢‘
            output_filename = os.path.join(OUTPUT_DIR, f"segment_{{segment_idx:04d}}.wav")
            fake_audio = fake_audios[0, 0].float().cpu().numpy()
            sf.write(output_filename, fake_audio, dac_model.sample_rate)

            logger.info(f"âœ… å·²ä¿å­˜: {{output_filename}}")

            # æ¸…ç†æ˜¾å­˜
            del codes, fake_audios, fake_audio
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç‰‡æ®µ {{segment_idx}} å¤±è´¥: {{e}}")
            import traceback
            traceback.print_exc()

    logger.info("\\nğŸ‰ æ‰¹é‡ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()
'''
        return script
