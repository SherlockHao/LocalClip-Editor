"""
æµ‹è¯•GPUæ˜¾å­˜æ£€æµ‹å’Œæ¨¡å‹é€‰æ‹©åŠŸèƒ½
"""
import sys
import os
import io

# å¼ºåˆ¶ UTF-8 è¾“å‡º
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from batch_retranslate import get_gpu_memory_gb, select_model_by_gpu


def test_gpu_detection():
    """æµ‹è¯•GPUæ˜¾å­˜æ£€æµ‹"""
    print("\n" + "="*60)
    print("æµ‹è¯•GPUæ˜¾å­˜æ£€æµ‹")
    print("="*60)

    available_memory = get_gpu_memory_gb()

    if available_memory > 0:
        print(f"\nâœ… GPUæ£€æµ‹æˆåŠŸï¼Œå¯ç”¨æ˜¾å­˜: {available_memory:.2f} GB")
    else:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°GPUæˆ–æ˜¾å­˜ä¸è¶³")

    return available_memory


def test_model_selection():
    """æµ‹è¯•æ¨¡å‹é€‰æ‹©é€»è¾‘"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ¨¡å‹é€‰æ‹©")
    print("="*60)

    # è·å–æ¨¡å‹ç›®å½•
    backend_dir = os.path.dirname(os.path.abspath(__file__))
    localclip_dir = os.path.dirname(backend_dir)
    workspace_dir = os.path.dirname(localclip_dir)
    ai_editing_dir = os.path.dirname(workspace_dir)
    models_dir = os.path.join(ai_editing_dir, "models")

    print(f"\næ¨¡å‹ç›®å½•: {models_dir}")

    # æ£€æŸ¥æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(models_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
        return None

    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    print("\nå¯ç”¨æ¨¡å‹:")
    available_models = []
    for model_name in ["Qwen3-4B-FP8", "Qwen3-4B", "Qwen3-1.7B"]:
        model_path = os.path.join(models_dir, model_name)
        exists = os.path.exists(model_path)
        status = "âœ“" if exists else "âœ—"
        print(f"  {status} {model_name}")
        if exists:
            available_models.append(model_name)

    if not available_models:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹")
        return None

    # æ‰§è¡Œæ¨¡å‹é€‰æ‹©
    print("\n" + "-"*60)
    selected_model = select_model_by_gpu(models_dir)
    print("-"*60)

    print(f"\nâœ… é€‰æ‹©çš„æ¨¡å‹è·¯å¾„: {selected_model}")
    print(f"   æ¨¡å‹åç§°: {os.path.basename(selected_model)}")

    return selected_model


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸ” å¼€å§‹æµ‹è¯•æ¨¡å‹é€‰æ‹©åŠŸèƒ½...\n")

    # æµ‹è¯•1: GPUæ£€æµ‹
    available_memory = test_gpu_detection()

    # æµ‹è¯•2: æ¨¡å‹é€‰æ‹©
    selected_model = test_model_selection()

    # æ€»ç»“
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)

    if available_memory > 0:
        print(f"âœ… GPUå¯ç”¨æ˜¾å­˜: {available_memory:.2f} GB")
    else:
        print("âš ï¸  GPUæ˜¾å­˜ä¸è¶³æˆ–ä¸å¯ç”¨")

    if selected_model:
        print(f"âœ… å·²é€‰æ‹©æ¨¡å‹: {os.path.basename(selected_model)}")

        # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ç»™å‡ºå»ºè®®
        model_name = os.path.basename(selected_model)
        if model_name == "Qwen3-4B-FP8":
            print("\nğŸ’¡ å»ºè®®: ä½¿ç”¨ Qwen3-4B-FP8 å¯ä»¥è·å¾—æœ€ä½³ç¿»è¯‘è´¨é‡")
        elif model_name == "Qwen3-4B":
            print("\nğŸ’¡ å»ºè®®: ä½¿ç”¨ Qwen3-4B å¯ä»¥è·å¾—å¾ˆå¥½çš„ç¿»è¯‘è´¨é‡")
        elif model_name == "Qwen3-1.7B":
            print("\nğŸ’¡ å»ºè®®: Qwen3-1.7B æ˜¯å›é€€æ¨¡å‹ï¼Œå¦‚éœ€æ›´å¥½è´¨é‡è¯·è€ƒè™‘å‡çº§GPU")
    else:
        print("âŒ æœªèƒ½é€‰æ‹©æ¨¡å‹")

    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*60 + "\n")


if __name__ == "__main__":
    main()
