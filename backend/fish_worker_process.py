"""
Worker è¿›ç¨‹æ¨¡å— - Layer 1 ä¼˜åŒ–
å®ç°æŒä¹…åŒ–æ¨¡å‹åŠ è½½å’Œä»»åŠ¡å¤„ç†

ä½œè€…ï¼šClaude (ç¬¬äºŒé˜¶æ®µä¼˜åŒ–)
"""
import os
import sys
import platform
import torch
import numpy as np
from typing import Dict, List, Tuple
from loguru import logger
from fish_batch_processor import BatchProcessor

# æ·»åŠ  fish-speech ç›®å½•åˆ° Python è·¯å¾„
if platform.system() == "Windows":
    FISH_SPEECH_DIR = os.environ.get("FISH_SPEECH_DIR", r"C:\workspace\ai_editing\fish-speech-win")
else:
    FISH_SPEECH_DIR = os.environ.get("FISH_SPEECH_DIR", "/Users/yiya_workstation/Documents/ai_editing/fish-speech")

# ç¡®ä¿ fish_speech æ¨¡å—å¯ä»¥è¢«å¯¼å…¥
if FISH_SPEECH_DIR not in sys.path:
    sys.path.insert(0, FISH_SPEECH_DIR)


# ==============================================================================
# å…¨å±€å˜é‡ - è¿›ç¨‹çº§æŒä¹…åŒ–æ¨¡å‹
# ==============================================================================
# åœ¨ worker è¿›ç¨‹é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½ï¼Œä¹‹åå¤ç”¨
_WORKER_MODELS = None
_WORKER_DEVICE = None


def _get_device() -> str:
    """
    æ£€æµ‹å¯ç”¨è®¾å¤‡

    Returns:
        è®¾å¤‡åç§°: "cuda", "mps", æˆ– "cpu"
    """
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"
    else:
        return "cpu"


def _load_models(checkpoint_path: str, device: str) -> Dict:
    """
    åŠ è½½ Fish-Speech æ¨¡å‹ï¼ˆDAC + Text2Semanticï¼‰

    Args:
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        device: è®¡ç®—è®¾å¤‡

    Returns:
        æ¨¡å‹å­—å…¸ï¼ŒåŒ…å«:
        - "dac": DAC è§£ç æ¨¡å‹
        - "text2semantic": Text2Semantic æ¨¡å‹
        - "decode_one_token": è§£ç å‡½æ•°
    """
    # å¯¼å…¥ Fish-Speech æ¨¡å—ï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…ä¸»è¿›ç¨‹åŠ è½½ï¼‰
    from fish_speech.models.dac.inference import load_model as load_dac_model
    from fish_speech.models.text2semantic.inference import init_model

    logger.info(f"ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ°è®¾å¤‡: {device}")

    # è®¾ç½®ç²¾åº¦
    if device == "cuda":
        precision = torch.bfloat16
    else:
        precision = torch.float32

    # åŠ è½½ DAC æ¨¡å‹
    logger.info("  ğŸ“¦ åŠ è½½ DAC æ¨¡å‹...")
    dac_model = load_dac_model(
        config_name="modded_dac_vq",
        checkpoint_path=os.path.join(checkpoint_path, "codec.pth"),
        device=device
    )
    logger.info("  âœ… DAC æ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½ Text2Semantic æ¨¡å‹
    logger.info("  ğŸ“¦ åŠ è½½ Text2Semantic æ¨¡å‹...")
    text2semantic_model, decode_one_token = init_model(
        checkpoint_path=checkpoint_path,
        device=device,
        precision=precision,
        compile=False  # MPS ä¸æ”¯æŒ torch.compile
    )
    logger.info("  âœ… Text2Semantic æ¨¡å‹åŠ è½½å®Œæˆ")

    return {
        "dac": dac_model,
        "text2semantic": text2semantic_model,
        "decode_one_token": decode_one_token
    }


def worker_process_main(
    worker_id: int,
    assigned_speaker_ids: List[int],
    tasks: List[Dict],
    speaker_npy_files: Dict[int, str],
    speaker_references: Dict[int, Dict],
    output_dir: str,
    checkpoint_path: str,
    batch_size: int = 10,
    io_threads: int = 2
) -> Dict[int, str]:
    """
    Worker è¿›ç¨‹ä¸»å‡½æ•° - å¤„ç†åˆ†é…çš„è¯´è¯äººä»»åŠ¡

    Args:
        worker_id: Worker è¿›ç¨‹ ID
        assigned_speaker_ids: åˆ†é…ç»™æ­¤ worker çš„è¯´è¯äºº ID åˆ—è¡¨
        tasks: æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ [{"speaker_id": int, "target_text": str, "segment_index": int}, ...]
        speaker_npy_files: è¯´è¯äºº npy æ–‡ä»¶å­—å…¸ {speaker_id: npy_path}
        speaker_references: è¯´è¯äººå‚è€ƒä¿¡æ¯å­—å…¸ {speaker_id: {"reference_text": str, ...}}
        output_dir: è¾“å‡ºç›®å½•
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        batch_size: DAC æ‰¹é‡è§£ç å¤§å°
        io_threads: I/O çº¿ç¨‹æ•°

    Returns:
        ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶å­—å…¸ {segment_index: file_path}
    """
    global _WORKER_MODELS, _WORKER_DEVICE

    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸš€ Worker #{worker_id} å¯åŠ¨")
    logger.info(f"åˆ†é…çš„è¯´è¯äºº: {assigned_speaker_ids}")
    logger.info(f"{'='*60}\n")

    # ==============================================================================
    # Step 1: åˆå§‹åŒ–æ¨¡å‹ï¼ˆé¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½ï¼Œä¹‹åå¤ç”¨ï¼‰
    # ==============================================================================
    if _WORKER_MODELS is None:
        logger.info(f"âš™ï¸  Worker #{worker_id}: é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨åŠ è½½æ¨¡å‹...")
        _WORKER_DEVICE = _get_device()
        _WORKER_MODELS = _load_models(checkpoint_path, _WORKER_DEVICE)
        logger.info(f"âœ… Worker #{worker_id}: æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯å¤ç”¨äºåç»­ä»»åŠ¡")
    else:
        logger.info(f"â™»ï¸  Worker #{worker_id}: å¤ç”¨å·²åŠ è½½çš„æ¨¡å‹")

    device = _WORKER_DEVICE

    # ==============================================================================
    # Step 2: åˆ›å»ºæ‰¹å¤„ç†å™¨
    # ==============================================================================
    processor = BatchProcessor(
        models=_WORKER_MODELS,
        batch_size=batch_size,
        io_threads=io_threads
    )

    # ==============================================================================
    # Step 3: å¤„ç†åˆ†é…çš„è¯´è¯äººä»»åŠ¡
    # ==============================================================================
    all_generated_files = {}

    for speaker_id in assigned_speaker_ids:
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¤ Worker #{worker_id}: å¤„ç†è¯´è¯äºº {speaker_id}")
        logger.info(f"{'='*60}")

        # æ£€æŸ¥ npy æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if speaker_id not in speaker_npy_files:
            logger.warning(f"âš ï¸  è¯´è¯äºº {speaker_id} æ²¡æœ‰ .npy æ–‡ä»¶ï¼Œè·³è¿‡")
            continue

        # è·å–è¯¥è¯´è¯äººçš„æ‰€æœ‰ä»»åŠ¡
        speaker_tasks = [t for t in tasks if t["speaker_id"] == speaker_id]
        if not speaker_tasks:
            logger.warning(f"âš ï¸  è¯´è¯äºº {speaker_id} æ²¡æœ‰åˆ†é…ä»»åŠ¡ï¼Œè·³è¿‡")
            continue

        logger.info(f"ğŸ“‹ è¯´è¯äºº {speaker_id} å…±æœ‰ {len(speaker_tasks)} ä¸ªç‰‡æ®µ")

        # æå–æ–‡æœ¬å’Œç´¢å¼•
        texts = [task["target_text"] for task in speaker_tasks]
        segment_indices = [task["segment_index"] for task in speaker_tasks]

        # åŠ è½½è¯¥è¯´è¯äººçš„ prompt tokens
        npy_file = speaker_npy_files[speaker_id]
        logger.info(f"ğŸ“‚ åŠ è½½ Prompt Tokens: {npy_file}")
        prompt_tokens = np.load(npy_file)
        prompt_tokens = torch.from_numpy(prompt_tokens).to(device).long()
        if prompt_tokens.ndim == 3:
            prompt_tokens = prompt_tokens[0]

        # è·å–å‚è€ƒæ–‡æœ¬
        ref_text = speaker_references[speaker_id]["reference_text"]

        # æ‰¹é‡ç”Ÿæˆè¯¥è¯´è¯äººçš„æ‰€æœ‰éŸ³é¢‘
        try:
            generated_files = processor.batch_generate_for_speaker(
                texts=texts,
                segment_indices=segment_indices,
                prompt_tokens=prompt_tokens,
                ref_text=ref_text,
                output_dir=output_dir,
                device=device,
                max_new_tokens=4096,
                top_p=0.7,
                temperature=0.7,
                repetition_penalty=1.2
            )

            # åˆå¹¶åˆ°æ€»ç»“æœ
            all_generated_files.update(generated_files)

            logger.info(
                f"âœ… Worker #{worker_id}: è¯´è¯äºº {speaker_id} "
                f"å®Œæˆ {len(generated_files)} ä¸ªç‰‡æ®µ"
            )

        except Exception as e:
            logger.error(
                f"âŒ Worker #{worker_id}: è¯´è¯äºº {speaker_id} å¤„ç†å¤±è´¥: {e}"
            )
            import traceback
            logger.error(traceback.format_exc())
            continue

    # ==============================================================================
    # Step 4: å…³é—­æ‰¹å¤„ç†å™¨ï¼ˆç­‰å¾…æ‰€æœ‰ I/O å®Œæˆï¼‰
    # ==============================================================================
    processor.shutdown()

    logger.info(f"\n{'='*60}")
    logger.info(
        f"ğŸ‰ Worker #{worker_id} å®Œæˆï¼å…±ç”Ÿæˆ {len(all_generated_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶"
    )
    logger.info(f"{'='*60}\n")

    return all_generated_files


# ==============================================================================
# è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ï¼ˆç”¨äº ProcessPoolExecutor çš„ initializerï¼‰
# ==============================================================================
def init_worker_process():
    """
    Worker è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°

    åœ¨æ¯ä¸ª worker è¿›ç¨‹å¯åŠ¨æ—¶è°ƒç”¨ï¼ˆä»…ä¸€æ¬¡ï¼‰
    è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡å’Œé…ç½®
    """
    # æ·»åŠ  fish-speech ç›®å½•åˆ° Python è·¯å¾„ï¼ˆå…³é”®ï¼ï¼‰
    fish_speech_dir = "/Users/yiya_workstation/Documents/ai_editing/fish-speech"
    if fish_speech_dir not in sys.path:
        sys.path.insert(0, fish_speech_dir)

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.add(
        sys.stderr,
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        level="INFO"
    )

    # ç¦ç”¨ tokenizers å¹¶è¡Œï¼ˆé¿å…å¤šè¿›ç¨‹è­¦å‘Šï¼‰
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    logger.info("ğŸ”§ Worker è¿›ç¨‹åˆå§‹åŒ–å®Œæˆ")
