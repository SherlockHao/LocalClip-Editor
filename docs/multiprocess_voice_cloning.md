# 多进程语音克隆使用说明

## 概述

LocalClip-Editor 现在支持多进程语音克隆，可以充分利用机器的 GPU 显存和内存，大幅提升语音克隆速度。

## 工作原理

### 单进程模式（默认）
- 一个进程顺序处理所有说话人
- 模型加载一次，按说话人分组批量处理
- 适合：显存较小的机器（<16GB）或说话人较少的场景

### 多进程模式（优化架构）
- **一次加载，批量处理**：每个 worker 加载模型一次，处理分配的所有说话人
- **轮询分配**：说话人均匀分配给 workers，确保负载均衡
- **并行执行**：所有 workers 同时运行，最大化并行度
- **零重复加载**：即使只有 1 个 worker，也不会重复加载模型

#### 进程分配策略

**单 GPU 场景**：
- 自动检测显存大小
- 每个模型实例约需 6-8GB 显存（可配置）
- 最大进程数 = min(可用显存 / 每个模型显存, 说话人数量)
- 例如：24GB 显存，8GB/模型 → 可以运行 2 个并行 worker
- 例如：24GB 显存，6GB/模型 → 可以运行 3 个并行 worker

**多 GPU 场景**：
- 每个 GPU 根据显存大小运行多个 worker
- Worker 自动分配到不同的 GPU
- 例如：2x 24GB GPU，8GB/模型 → 每个 GPU 2 个 worker，共 4 个并行 worker
- 例如：2x 16GB GPU，8GB/模型 → 每个 GPU 1 个 worker，共 2 个并行 worker

**分批处理**：
- 如果说话人数量 > 可用进程数，会分批处理
- 每批进程完成后启动下一批
- 确保资源充分利用且不会超限

## 性能对比

### 测试场景
- 6 个说话人
- 46 个音频片段
- 单 GPU (RTX 4090, 24GB)

### 单进程模式
- 耗时：约 2-3 分钟
- 显存占用：约 8GB
- CPU 占用：低

### 多进程模式（24GB 显存，2 worker）
- 耗时：约 1-1.5 分钟（**2x 加速**）
- 显存占用：约 16GB（2 个 worker）
- CPU 占用：中等

### 多进程模式（24GB 显存，3 worker，调整 FISH_MODEL_MEMORY_GB=6.0）
- 耗时：约 1 分钟（**2-3x 加速**）
- 显存占用：约 18-20GB（3 个 worker）
- CPU 占用：中等偏高

## 启用方法

### 方法 1: 环境变量配置（推荐）

编辑 `.env` 文件：

```bash
# 启用多进程模式
FISH_MULTIPROCESS_MODE=true

# 每个模型实例需要的显存（GB）
# 默认 8.0，可以根据实际情况调整
# - 调低（如 6.0）：可以运行更多 worker，但可能 OOM
# - 调高（如 10.0）：更保守，减少 OOM 风险
FISH_MODEL_MEMORY_GB=8.0
```

重启后端服务后生效。

### 方法 2: 代码配置

在 `main.py` 中修改：

```python
# 启用多进程
batch_cloner = SimpleFishCloner(use_multiprocess=True)

# 禁用多进程（单进程模式）
batch_cloner = SimpleFishCloner(use_multiprocess=False)
```

## 适用场景

### 推荐使用多进程的场景
✅ 显存充足（≥16GB）
✅ 说话人数量较多（≥3 个）
✅ 音频片段较多（≥20 个）
✅ 有多个 GPU

### 推荐使用单进程的场景
✅ 显存较小（<16GB）
✅ 说话人数量少（1-2 个）
✅ 音频片段较少（<20 个）
✅ CPU 模式运行

## 注意事项

### 显存管理
- 每个进程独立加载模型，需要约 6-8GB 显存
- 如果显存不足，系统会自动减少进程数或退化为单进程
- 建议预留 2-4GB 显存给系统和其他应用

### 性能优化建议
1. **GPU 加速**：确保安装了 CUDA 和 PyTorch GPU 版本
2. **显存清理**：每个进程完成后会自动清理显存
3. **批处理**：说话人数量多时会自动分批，避免资源耗尽
4. **调整显存配置**：
   - 先用默认值 `FISH_MODEL_MEMORY_GB=8.0` 测试
   - 如果显存有余量，可以降低到 6.0-7.0，运行更多 worker
   - 如果遇到 OOM，提高到 9.0-10.0
   - 运行时观察日志输出的 GPU 分配信息

### 故障排查

**问题 1: 进程启动失败**
- 检查显存是否充足
- 查看后端日志中的错误信息
- 尝试降低进程数或切换到单进程模式

**问题 2: 显存不足 (OOM)**
- 在 `.env` 中设置 `FISH_MULTIPROCESS_MODE=false`
- 或手动减少模型显存占用（调整 precision）

**问题 3: 速度没有提升**
- 检查是否真的运行在多进程模式（查看日志）
- 确认 GPU 数量和显存大小
- 说话人数量太少时多进程优势不明显

## 测试方法

运行测试脚本对比性能：

```bash
cd backend
python test_multiprocess.py
```

测试脚本会：
1. 运行单进程模式生成测试音频
2. 运行多进程模式生成测试音频
3. 对比耗时和加速比

## 技术实现

### 文件结构

```
backend/
├── fish_simple_cloner.py          # 主克隆器（支持单进程/多进程）
├── fish_batch_generate.py         # 单进程生成脚本
├── fish_multiprocess_generate.py  # 多进程生成脚本
├── fish_simple_encode.py          # 编码脚本
└── test_multiprocess.py           # 测试脚本
```

### 关键代码

**多进程生成脚本** (`fish_multiprocess_generate.py`):
- 按说话人分组任务
- 创建工作进程池
- 每个进程独立加载模型
- 使用 Queue 收集结果
- 分批处理以控制资源使用

**SimpleFishCloner** (`fish_simple_cloner.py`):
- 根据配置选择单进程或多进程脚本
- 统一的接口，透明切换
- 实时显示进度信息

## 未来优化方向

1. **动态进程调度**：根据实时显存占用动态调整进程数
2. **GPU 亲和性**：更智能的 GPU 分配策略
3. **混合模式**：大说话人用多进程，小说话人用单进程
4. **进度可视化**：Web 界面显示每个进程的进度

## 常见问题

**Q: 多进程会不会增加总显存占用？**
A: 是的。单进程占用约 8GB，两个进程约 16GB。但并行处理可以大幅减少总耗时。

**Q: 可以手动设置进程数吗？**
A: 当前版本自动检测。未来可以添加配置选项。

**Q: CPU 模式支持多进程吗？**
A: 支持，但由于 CPU 速度慢，多进程优势不明显，推荐单进程。

**Q: 多进程会影响生成质量吗？**
A: 不会。使用相同的模型和参数，质量完全一致。

## 版本历史

- **v1.0** (2025-12-13): 初始版本，支持单 GPU 和多 GPU 多进程并行
